{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "5df12084-9b11-497c-88bb-99ba5531f370",
   "metadata": {},
   "source": [
    "# Contents of the notebook"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "57635ebb-f017-4e77-b577-2aa8ad13c989",
   "metadata": {},
   "source": [
    "To be able to use the packages ``bw2regional`` and ``bw2_lcimpact``, I modified their code. I did not ask for a pulling request on Chris Mutel's Github. Therefore, this notebook is temporary and contains the code that you should include in the packages of your environment to make this tutorial work. You can copy past it instead of the code you downloaded from Chris Mutel repository."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e144b3ab-7cd6-4ed9-93df-7a736f9eec7e",
   "metadata": {},
   "source": [
    "The objectives of this notebook are:\n",
    "1. To modify the code of bw2data. The adress is:\n",
    "   ``C:\\Users\\``yourname``\\AppData\\Local\\anaconda3\\envs\\brightway_regional_2\\Lib\\site-packages\\bw2data``\n",
    "2. To modify the code of bw2regional. The adress is:\n",
    "   ``C:\\Users\\``yourname``\\AppData\\Local\\anaconda3\\envs\\brightway_regional_2\\Lib\\site-packages\\bw2regional``\n",
    "3. To modify the code of bw2_lcimpact. The adress is:\n",
    "   ``C:\\Users\\``yourname``\\AppData\\Local\\anaconda3\\envs\\brightway_regional_2\\Lib\\site-packages\\bw2_lcimpact``"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0c259a75-b681-4230-b64e-28d4d5aa9ef3",
   "metadata": {},
   "source": [
    "# Modification of the bw2data code"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a8b51da6-641a-4d2f-af08-f36ed2e1d75a",
   "metadata": {},
   "source": [
    "## utils.py"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "250aff4e-77c8-45a7-aaba-0a68aa4ad389",
   "metadata": {},
   "source": [
    "For the ``get_geocollection(location, default_global_location=False):`` function, I removed attaching the geocollection ``World`` to the ``glo`` location. In fact, it leads to \"global cfs\" being regionalised although they are not. Further, it leads brightway regional to calculate un-necessary intersections."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "be5ae090-93b7-4bda-a407-7189f2d16c68",
   "metadata": {},
   "outputs": [],
   "source": [
    "import collections\n",
    "import itertools\n",
    "import numbers\n",
    "import os\n",
    "import random\n",
    "import re\n",
    "import string\n",
    "import urllib\n",
    "import warnings\n",
    "import zipfile\n",
    "from io import StringIO\n",
    "from pathlib import Path\n",
    "\n",
    "import stats_arrays as sa\n",
    "\n",
    "from .configuration import labels\n",
    "from .errors import MultipleResults, NotFound, UnknownObject, ValidityError\n",
    "from .fatomic import open\n",
    "\n",
    "DOWNLOAD_URL = \"https://brightway.dev/data/\"\n",
    "\n",
    "\n",
    "def safe_filename(*args, **kwargs):\n",
    "    raise DeprecationWarning(\"`safe_filename` has been moved to `bw_processing`\")\n",
    "\n",
    "\n",
    "def maybe_path(x):\n",
    "    return Path(x) if x else x\n",
    "\n",
    "\n",
    "def natural_sort(l):\n",
    "    \"\"\"Sort the given list in the way that humans expect, e.g. 9 before 10.\"\"\"\n",
    "    # http://nedbatchelder.com/blog/200712/human_sorting.html#comments\n",
    "    convert = lambda text: int(text) if text.isdigit() else text.lower()\n",
    "    alphanum_key = lambda key: [convert(c) for c in re.split(\"([0-9]+)\", key)]\n",
    "    return sorted(l, key=alphanum_key)\n",
    "\n",
    "\n",
    "def random_string(length=8):\n",
    "    \"\"\"Generate a random string of letters and numbers.\n",
    "\n",
    "    Args:\n",
    "        * *length* (int): Length of string, default is 8\n",
    "\n",
    "    Returns:\n",
    "        A string (not unicode)\n",
    "\n",
    "    \"\"\"\n",
    "    return \"\".join(\n",
    "        random.choice(string.ascii_letters + string.digits) for i in range(length)\n",
    "    )\n",
    "\n",
    "\n",
    "def combine_methods(name, *ms):\n",
    "    \"\"\"Combine LCIA methods by adding duplicate characterization factors.\n",
    "\n",
    "    Args:\n",
    "        * *ms* (one or more method id tuples): Any number of method ids, e.g.\n",
    "        ``(\"my method\", \"wow\"), (\"another method\", \"wheee\")``.\n",
    "\n",
    "    Returns:\n",
    "        The new Method instance.\n",
    "\n",
    "    \"\"\"\n",
    "    from . import Method, methods\n",
    "\n",
    "    data = {}\n",
    "    units = set([methods[tuple(x)][\"unit\"] for x in ms])\n",
    "    for m in ms:\n",
    "        for key, cf, geo in Method(m).load():\n",
    "            data[(key, geo)] = data.get((key, geo), 0) + cf\n",
    "    meta = {\n",
    "        \"description\": \"Combination of the following methods: \"\n",
    "        + \", \".join([str(x) for x in ms]),\n",
    "        \"unit\": list(units)[0] if len(units) == 1 else \"Unknown\",\n",
    "    }\n",
    "    data = [(key, cf, geo) for (key, geo), cf in data.items()]\n",
    "    method = Method(name)\n",
    "    method.register(**meta)\n",
    "    method.write(data)\n",
    "    return method\n",
    "\n",
    "\n",
    "def clean_exchanges(data):\n",
    "    \"\"\"Make sure all exchange inputs are tuples, not lists.\"\"\"\n",
    "\n",
    "    def tupleize(value):\n",
    "        for exc in value.get(\"exchanges\", []):\n",
    "            exc[\"input\"] = tuple(exc[\"input\"])\n",
    "        return value\n",
    "\n",
    "    return {key: tupleize(value) for key, value in data.items()}\n",
    "\n",
    "\n",
    "POSITIVE_DISTRIBUTIONS = {2, 6, 8, 9, 10}\n",
    "\n",
    "\n",
    "def as_uncertainty_dict(value):\n",
    "    \"\"\"Given either a number or a ``stats_arrays`` uncertainty dict, return an uncertainty dict\"\"\"\n",
    "    if isinstance(value, dict):\n",
    "        if (\n",
    "            value.get(\"amount\", 0) < 0\n",
    "            and (\n",
    "                value.get(\"uncertainty_type\") in POSITIVE_DISTRIBUTIONS\n",
    "                or value.get(\"uncertainty type\") in POSITIVE_DISTRIBUTIONS\n",
    "            )\n",
    "            and \"negative\" not in value\n",
    "        ):\n",
    "            value[\"negative\"] = True\n",
    "        return value\n",
    "    try:\n",
    "        return {\"amount\": float(value)}\n",
    "    except:\n",
    "        raise TypeError(\n",
    "            \"Value must be either an uncertainty dict. or number\"\n",
    "            \" (got %s: %s)\" % (type(value), value)\n",
    "        )\n",
    "\n",
    "\n",
    "def uncertainify(data, distribution=None, bounds_factor=0.1, sd_factor=0.1):\n",
    "    \"\"\"\n",
    "    Add some rough uncertainty to exchanges.\n",
    "\n",
    "    .. warning:: This function only changes exchanges with no uncertainty type or uncertainty type ``UndefinedUncertainty``, and does not change production exchanges!\n",
    "\n",
    "    Can only apply normal or uniform uncertainty distributions; default is uniform. Distribution, if specified, must be a ``stats_array`` uncertainty object.\n",
    "\n",
    "    ``data`` is a LCI data dictionary.\n",
    "\n",
    "    If using the normal distribution:\n",
    "\n",
    "    * ``sd_factor`` will be multiplied by the mean to calculate the standard deviation.\n",
    "    * If no bounds are desired, set ``bounds_factor`` to ``None``.\n",
    "    * Otherwise, the bounds will be ``[(1 - bounds_factor) * mean, (1 + bounds_factor) * mean]``.\n",
    "\n",
    "    If using the uniform distribution, then the bounds are ``[(1 - bounds_factor) * mean, (1 + bounds_factor) * mean]``.\n",
    "\n",
    "    Returns the modified data.\n",
    "    \"\"\"\n",
    "    assert distribution in {\n",
    "        None,\n",
    "        sa.UniformUncertainty,\n",
    "        sa.NormalUncertainty,\n",
    "    }, \"``uncertainify`` only supports normal and uniform distributions\"\n",
    "    assert (\n",
    "        bounds_factor is None or bounds_factor * 1.0 > 0\n",
    "    ), \"bounds_factor must be a positive number\"\n",
    "    assert sd_factor * 1.0 > 0, \"sd_factor must be a positive number\"\n",
    "\n",
    "    for key, value in data.items():\n",
    "        for exchange in value.get(\"exchanges\", []):\n",
    "            if (exchange.get(\"type\") in labels.technosphere_positive_edge_types) or (\n",
    "                exchange.get(\"uncertainty type\", sa.UndefinedUncertainty.id)\n",
    "                != sa.UndefinedUncertainty.id\n",
    "            ):\n",
    "                continue\n",
    "            if exchange[\"amount\"] == 0:\n",
    "                continue\n",
    "\n",
    "            if bounds_factor is not None:\n",
    "                exchange.update(\n",
    "                    {\n",
    "                        \"minimum\": (1 - bounds_factor) * exchange[\"amount\"],\n",
    "                        \"maximum\": (1 + bounds_factor) * exchange[\"amount\"],\n",
    "                    }\n",
    "                )\n",
    "                if exchange[\"amount\"] < 0:\n",
    "                    exchange[\"minimum\"], exchange[\"maximum\"] = (\n",
    "                        exchange[\"maximum\"],\n",
    "                        exchange[\"minimum\"],\n",
    "                    )\n",
    "\n",
    "            if distribution == sa.NormalUncertainty:\n",
    "                exchange.update(\n",
    "                    {\n",
    "                        \"uncertainty type\": sa.NormalUncertainty.id,\n",
    "                        \"loc\": exchange[\"amount\"],\n",
    "                        \"scale\": abs(sd_factor * exchange[\"amount\"]),\n",
    "                    }\n",
    "                )\n",
    "            else:\n",
    "                assert (\n",
    "                    bounds_factor is not None\n",
    "                ), \"must specify bounds_factor for uniform distribution\"\n",
    "                exchange.update(\n",
    "                    {\n",
    "                        \"uncertainty type\": sa.UniformUncertainty.id,\n",
    "                    }\n",
    "                )\n",
    "    return data\n",
    "\n",
    "\n",
    "def recursive_str_to_unicode(data, encoding=\"utf8\"):\n",
    "    \"\"\"Convert the strings inside a (possibly nested) python data structure to unicode strings using `encoding`.\"\"\"\n",
    "    # Adapted from\n",
    "    # http://stackoverflow.com/questions/1254454/fastest-way-to-convert-a-dicts-keys-values-from-unicode-to-str\n",
    "    if isinstance(data, str):\n",
    "        return data\n",
    "    elif isinstance(data, bytes):\n",
    "        return str(data, encoding)  # Faster than str.encode\n",
    "    elif isinstance(data, collections.abc.Mapping):\n",
    "        return dict(\n",
    "            map(recursive_str_to_unicode, data.items(), itertools.repeat(encoding))\n",
    "        )\n",
    "    elif isinstance(data, collections.abc.Iterable):\n",
    "        return type(data)(\n",
    "            map(recursive_str_to_unicode, data, itertools.repeat(encoding))\n",
    "        )\n",
    "    else:\n",
    "        return data\n",
    "\n",
    "\n",
    "def combine_databases(name, *dbs):\n",
    "    \"\"\"Combine databases into new database called ``name``.\"\"\"\n",
    "    pass\n",
    "\n",
    "\n",
    "def merge_databases(parent_db, other):\n",
    "    \"\"\"Merge ``other`` into ``parent_db``, including updating exchanges.\n",
    "\n",
    "    All databases must be SQLite databases.\n",
    "\n",
    "    ``parent_db`` and ``other`` should be the names of databases.\n",
    "\n",
    "    Doesn't return anything.\"\"\"\n",
    "    from . import databases\n",
    "    from .backends import (\n",
    "        ActivityDataset,\n",
    "        ExchangeDataset,\n",
    "        SQLiteBackend,\n",
    "        sqlite3_lci_db,\n",
    "    )\n",
    "    from .database import Database\n",
    "\n",
    "    assert parent_db in databases\n",
    "    assert other in databases\n",
    "\n",
    "    first = Database(parent_db)\n",
    "    second = Database(other)\n",
    "\n",
    "    if type(first) != SQLiteBackend or type(second) != SQLiteBackend:\n",
    "        raise ValidityError(\"Both databases must be `SQLiteBackend`\")\n",
    "\n",
    "    first_codes = {\n",
    "        obj.code\n",
    "        for obj in ActivityDataset.select().where(ActivityDataset.database == parent_db)\n",
    "    }\n",
    "    second_codes = {\n",
    "        obj.code\n",
    "        for obj in ActivityDataset.select().where(ActivityDataset.database == other)\n",
    "    }\n",
    "    if first_codes.intersection(second_codes):\n",
    "        raise ValidityError(\"Duplicate codes - can't merge databases\")\n",
    "\n",
    "    with sqlite3_lci_db.atomic():\n",
    "        ActivityDataset.update(database=parent_db).where(\n",
    "            ActivityDataset.database == other\n",
    "        ).execute()\n",
    "        ExchangeDataset.update(input_database=parent_db).where(\n",
    "            ExchangeDataset.input_database == other\n",
    "        ).execute()\n",
    "        ExchangeDataset.update(output_database=parent_db).where(\n",
    "            ExchangeDataset.output_database == other\n",
    "        ).execute()\n",
    "\n",
    "    Database(parent_db).process()\n",
    "    del databases[other]\n",
    "\n",
    "\n",
    "def download_file(filename, directory=\"downloads\", url=None):\n",
    "    \"\"\"Download a file and write it to disk in ``downloads`` directory.\n",
    "\n",
    "    If ``url`` is None, uses the Brightway2 data base URL. ``url`` should everything up to the filename, such that ``url`` + ``filename`` is the valid complete URL to download from.\n",
    "\n",
    "    Streams download to reduce memory usage.\n",
    "\n",
    "    Args:\n",
    "        * *filename* (str): The filename to download.\n",
    "        * *directory* (str, optional): Directory to save the file. Created if it doesn't already exist.\n",
    "        * *url* (str, optional): URL where the file is located, if not the default Brightway data URL.\n",
    "\n",
    "    Returns:\n",
    "        The path of the created file.\n",
    "\n",
    "    \"\"\"\n",
    "    from . import projects\n",
    "\n",
    "    assert isinstance(directory, str), \"`directory` must be a string\"\n",
    "    dirpath = projects.request_directory(directory)\n",
    "    filepath = dirpath / filename\n",
    "    download_path = (url if url is not None else DOWNLOAD_URL) + filename\n",
    "    with urllib.request.urlopen(download_path) as response, open(\n",
    "        filepath, \"wb\"\n",
    "    ) as out_file:\n",
    "        if response.status != 200:\n",
    "            raise NotFound(\n",
    "                \"URL {} returns status code {}.\".format(download_path, response.status)\n",
    "            )\n",
    "        chunk = 128 * 1024\n",
    "        while True:\n",
    "            segment = response.read(chunk)\n",
    "            if not segment:\n",
    "                break\n",
    "            out_file.write(segment)\n",
    "    return filepath\n",
    "\n",
    "\n",
    "def set_data_dir(dirpath, permanent=True):\n",
    "    \"\"\"Set the Brightway2 data directory to ``dirpath``.\n",
    "\n",
    "    If ``permanent`` is ``True``, then set ``dirpath`` as the default data directory.\n",
    "\n",
    "    Creates ``dirpath`` if needed. Also creates basic directories, and resets metadata.\n",
    "\n",
    "    \"\"\"\n",
    "    warnings.warn(\n",
    "        \"`set_data_dir` is deprecated; use `projects.set_current('my \"\n",
    "        \"project name')` for a new project space.\",\n",
    "        DeprecationWarning,\n",
    "    )\n",
    "\n",
    "\n",
    "def switch_data_directory(dirpath):\n",
    "    from .projects import ProjectDataset, SubstitutableDatabase\n",
    "\n",
    "    if dirpath == bw.projects._base_data_dir:\n",
    "        print(\"dirpath already loaded\")\n",
    "        return\n",
    "    try:\n",
    "        assert os.path.isdir(dirpath)\n",
    "        bw.projects._base_data_dir = dirpath\n",
    "        bw.projects._base_logs_dir = os.path.join(dirpath, \"logs\")\n",
    "        # create folder if it does not yet exist\n",
    "        if not os.path.isdir(bw.projects._base_logs_dir):\n",
    "            os.mkdir(bw.projects._base_logs_dir)\n",
    "        # load new brightway directory\n",
    "        bw.projects.db = SubstitutableDatabase(\n",
    "            os.path.join(bw.projects._base_data_dir, \"projects.db\"), [ProjectDataset]\n",
    "        )\n",
    "        print(\"Loaded brightway2 data directory: {}\".format(bw.projects._base_data_dir))\n",
    "\n",
    "    except AssertionError:\n",
    "        print('Could not access directory specified \"dirpath\"')\n",
    "\n",
    "\n",
    "def create_in_memory_zipfile_from_directory(path):\n",
    "    # Based on http://stackoverflow.com/questions/2463770/python-in-memory-zip-library\n",
    "    memory_obj = StringIO()\n",
    "    files = [f for f in os.listdir(path) if os.path.isfile(os.path.join(path, f))]\n",
    "    zf = zipfile.ZipFile(memory_obj, \"a\", zipfile.ZIP_DEFLATED, False)\n",
    "    for filename in files:\n",
    "        zf.writestr(filename, open(os.path.join(path, filename)).read())\n",
    "    # Mark the files as having been created on Windows so that\n",
    "    # Unix permissions are not inferred as 0000\n",
    "    for zfile in zf.filelist:\n",
    "        zfile.create_system = 0\n",
    "    zf.close()\n",
    "    memory_obj.seek(0)\n",
    "    return memory_obj\n",
    "\n",
    "\n",
    "def get_node(**kwargs):\n",
    "    from . import databases\n",
    "    from .backends import ActivityDataset as AD\n",
    "    from .subclass_mapping import NODE_PROCESS_CLASS_MAPPING\n",
    "\n",
    "    def node_class(database_name):\n",
    "        return NODE_PROCESS_CLASS_MAPPING[\n",
    "            databases[database_name].get(\"backend\", \"sqlite\")\n",
    "        ]\n",
    "\n",
    "    mapping = {\n",
    "        \"id\": AD.id,\n",
    "        \"code\": AD.code,\n",
    "        \"database\": AD.database,\n",
    "        \"location\": AD.location,\n",
    "        \"name\": AD.name,\n",
    "        \"product\": AD.product,\n",
    "        \"type\": AD.type,\n",
    "    }\n",
    "\n",
    "    qs = AD.select()\n",
    "    for key, value in kwargs.items():\n",
    "        try:\n",
    "            qs = qs.where(mapping[key] == value)\n",
    "        except KeyError:\n",
    "            continue\n",
    "\n",
    "    candidates = [node_class(obj.database)(obj) for obj in qs]\n",
    "\n",
    "    extended_search = any(key not in mapping for key in kwargs)\n",
    "    if extended_search:\n",
    "        if \"database\" not in kwargs:\n",
    "            warnings.warn(\n",
    "                \"Given search criteria very broad; try to specify at least a database\"\n",
    "            )\n",
    "        candidates = [\n",
    "            obj\n",
    "            for obj in candidates\n",
    "            if all(\n",
    "                obj.get(key) == value\n",
    "                for key, value in kwargs.items()\n",
    "                if key not in mapping\n",
    "            )\n",
    "        ]\n",
    "    if len(candidates) > 1:\n",
    "        raise MultipleResults(\n",
    "            \"Found {} results for the given search\".format(len(candidates))\n",
    "        )\n",
    "    elif not candidates:\n",
    "        raise UnknownObject\n",
    "    return candidates[0]\n",
    "\n",
    "\n",
    "def get_activity(key=None, **kwargs):\n",
    "    \"\"\"Support multiple ways to get exactly one activity node.\n",
    "\n",
    "    ``key`` can be an integer or a key tuple.\"\"\"\n",
    "    from .backends import Activity\n",
    "\n",
    "    # Includes subclasses\n",
    "    if isinstance(key, Activity):\n",
    "        return key\n",
    "    elif isinstance(key, tuple):\n",
    "        kwargs[\"database\"] = key[0]\n",
    "        kwargs[\"code\"] = key[1]\n",
    "    elif isinstance(key, numbers.Integral):\n",
    "        kwargs[\"id\"] = key\n",
    "    return get_node(**kwargs)\n",
    "\n",
    "\n",
    "def get_geocollection(location, default_global_location=False):\n",
    "    \"\"\"conservative approach to finding geocollections. Won't guess about ecoinvent or other databases.\"\"\"\n",
    "    if not location:\n",
    "        if default_global_location:\n",
    "            return \"world\"\n",
    "        else:\n",
    "            return None\n",
    "    elif isinstance(location, tuple):\n",
    "        return location[0]\n",
    "    #elif isinstance(location, str) and (\n",
    "    #    len(location) == 2 or location.lower() == \"glo\"\n",
    "    #):\n",
    "    #    return \"world\"\n",
    "    else:\n",
    "        return None"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1b51a479-fb44-4218-8cc4-6799ddcbc5d8",
   "metadata": {},
   "source": [
    "## method.py"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "68ce5a56-6262-461a-99e7-601e066fc4a1",
   "metadata": {},
   "source": [
    "I used the ``false`` condition instead of the ``true`` one in ``get_geocollection(third(elem), default_global_location=False)``. In fact, the methods are not necessarily regionalized. What's more, sometimes, they are assigned by default the ``world`` location, eventhough they should be linked to another geocollection. It leads to brightway regional calculating intersections when it is not necessary."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "23197521-fbf9-440d-97b0-37396f2de338",
   "metadata": {},
   "outputs": [],
   "source": [
    "from . import config, geomapping, methods\n",
    "from .backends.schema import get_id\n",
    "from .errors import UnknownObject\n",
    "from .ia_data_store import ImpactAssessmentDataStore\n",
    "from .utils import as_uncertainty_dict, get_geocollection\n",
    "from .validate import ia_validator\n",
    "\n",
    "\n",
    "class Method(ImpactAssessmentDataStore):\n",
    "    \"\"\"A manager for an impact assessment method. This class can register or deregister methods, write intermediate data, process data to parameter arrays, validate, and copy methods.\n",
    "\n",
    "    The Method class never holds intermediate data, but it can load or write intermediate data. The only attribute is *name*, which is the name of the method being managed.\n",
    "\n",
    "    Instantiation does not load any data. If this method is not yet registered in the metadata store, a warning is written to ``stdout``.\n",
    "\n",
    "    Methods are hierarchally structured, and this structure is preserved in the method name. It is a tuple of strings, like ``('ecological scarcity 2006', 'total', 'natural resources')``.\n",
    "\n",
    "    The data schema for IA methods is:\n",
    "\n",
    "    .. code-block:: python\n",
    "\n",
    "            Schema([Any(\n",
    "                [valid_tuple, maybe_uncertainty],         # site-generic\n",
    "                [valid_tuple, maybe_uncertainty, object]  # regionalized\n",
    "            )])\n",
    "\n",
    "    where:\n",
    "        * *valid_tuple* (tuple): A dataset identifier, like ``(\"biosphere\", \"CO2\")``.\n",
    "        * *maybe_uncertainty* (uncertainty dict or number): Either a number or an uncertainty dictionary.\n",
    "        * *object* (object, optional) is a location identifier, used only for regionalized LCIA.\n",
    "\n",
    "    Args:\n",
    "        * *name* (tuple): Name of impact assessment method to manage.\n",
    "\n",
    "    \"\"\"\n",
    "\n",
    "    _metadata = methods\n",
    "    validator = ia_validator\n",
    "    matrix = \"characterization_matrix\"\n",
    "\n",
    "    def add_geomappings(self, data):\n",
    "        geomapping.add({x[2] for x in data if len(x) == 3})\n",
    "\n",
    "    def process_row(self, row):\n",
    "        \"\"\"Given ``(flow, amount, maybe location)``, return a dictionary for array insertion.\"\"\"\n",
    "        try:\n",
    "            return {\n",
    "                **as_uncertainty_dict(row[1]),\n",
    "                \"row\": get_id(row[0]),\n",
    "                \"col\": (\n",
    "                    geomapping[row[2]]\n",
    "                    if len(row) >= 3\n",
    "                    else geomapping[config.global_location]\n",
    "                ),\n",
    "            }\n",
    "        except UnknownObject:\n",
    "            raise UnknownObject(\n",
    "                \"Can't find flow `{}`, specified in CF row `{}` for method `{}`\".format(\n",
    "                    {tuple(row[0]) if isinstance(row[0], list) else row[0]}, row, self.name\n",
    "                )\n",
    "            )\n",
    "        except KeyError:\n",
    "            if len(row) >= 3 and row[2] not in geomapping:\n",
    "                raise UnknownObject(\n",
    "                    \"Can't find location `{}`, specified in CF row `{}` for method `{}`\".format(\n",
    "                        {row[2]}, row, self.name\n",
    "                    )\n",
    "                )\n",
    "            elif config.global_location not in geomapping:\n",
    "                raise UnknownObject(\n",
    "                    \"Can't find default global location! It's supposed to be `{}`, but this isn't in the `geomapping`\".format(\n",
    "                        config.global_location\n",
    "                    )\n",
    "                )\n",
    "\n",
    "    def write(self, data, process=True):\n",
    "        \"\"\"Serialize intermediate data to disk.\n",
    "\n",
    "        Sets the metadata key ``num_cfs`` automatically.\"\"\"\n",
    "        if self.name not in self._metadata:\n",
    "            self.register()\n",
    "        self.metadata[\"num_cfs\"] = len(data)\n",
    "\n",
    "        third = lambda x: x[2] if len(x) == 3 else None\n",
    "\n",
    "        geocollections = {\n",
    "            get_geocollection(third(elem), default_global_location=False) ## NR : I changed this because else, it will attach a \"world\" geocollection to the LCIA eventhough the methods are not necessarily regionalized.\n",
    "            for elem in data\n",
    "        }\n",
    "        if None in geocollections:\n",
    "            geocollections.discard(None)\n",
    "\n",
    "        self.metadata[\"geocollections\"] = sorted(geocollections)\n",
    "        self._metadata.flush()\n",
    "        super(Method, self).write(data, process=process)\n",
    "\n",
    "    def process(self, **extra_metadata):\n",
    "        try:\n",
    "            extra_metadata[\"global_index\"] = geomapping[config.global_location]\n",
    "        except KeyError:\n",
    "            raise KeyError(\n",
    "                \"Can't find default global location! It's supposed to be `{}`, defined in `config`, but this isn't in the `geomapping`\".format(\n",
    "                    config.global_location\n",
    "                )\n",
    "            )\n",
    "        super().process(**extra_metadata)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f8eef430-619e-456d-b957-d2a61412d81e",
   "metadata": {},
   "source": [
    "# Modification of the ``bw2regional`` code"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "03a6bd89-4086-4711-957b-a738df1e6972",
   "metadata": {},
   "source": [
    "## __init__.py"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "838c2a22-0f06-4b6e-8511-ea448a5d9887",
   "metadata": {},
   "source": [
    "I added the importation of the new ``GlobalLocLCA``, allowing to compute the impacts of elementary flows of activities in the [GLO] location."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1039a232-a5f5-4495-9062-c7b1da3046f2",
   "metadata": {},
   "outputs": [],
   "source": [
    "__all__ = (\n",
    "    \"cg\",\n",
    "    \"calculate_needed_intersections\",\n",
    "    \"create_ecoinvent_collections\",\n",
    "    \"create_empty_intersection\",\n",
    "    \"create_restofworlds_collections\",\n",
    "    \"create_world_collections\",\n",
    "    \"divide_by_area\",\n",
    "    \"extension_tables\",\n",
    "    \"ExtensionTable\",\n",
    "    \"ExtensionTablesLCA\",\n",
    "    \"geocollections\",\n",
    "    \"get_spatial_dataset_kind\",\n",
    "    \"GlobalLocLCA\", # NR : I added this line to compute the impacts of elementary flows of activities in the [GLO] location.\n",
    "    \"hash_collection\",\n",
    "    \"import_from_pandarus\",\n",
    "    \"import_regionalized_cfs\",\n",
    "    \"Intersection\",\n",
    "    \"calculate_intersection\",\n",
    "    \"intersections\",\n",
    "    \"label_activity_geocollections\",\n",
    "    \"Loading\",\n",
    "    \"loadings\",\n",
    "    \"OneSpatialScaleLCA\",\n",
    "    \"PandarusRemote\",\n",
    "    \"remote\",\n",
    "    \"reset_all_geo\",\n",
    "    \"reset_geo_meta\",\n",
    "    \"sha256\",\n",
    "    \"topocollections\",\n",
    "    \"Topography\",\n",
    "    \"TwoSpatialScalesLCA\",\n",
    "    \"TwoSpatialScalesWithGenericLoadingLCA\",\n",
    "    \"raster_as_extension_table\",\n",
    ")\n",
    "\n",
    "# ignore future warning from pandas that we can't fix\n",
    "import warnings\n",
    "\n",
    "from .version import version as __version__\n",
    "\n",
    "warnings.simplefilter(action=\"ignore\", category=FutureWarning)\n",
    "\n",
    "from constructive_geometries import ConstructiveGeometries\n",
    "\n",
    "cg = ConstructiveGeometries()\n",
    "\n",
    "from bw2data import config\n",
    "\n",
    "from .topography import Topography\n",
    "from .loading import Loading\n",
    "from .meta import (\n",
    "    extension_tables,\n",
    "    geocollections,\n",
    "    intersections,\n",
    "    loadings,\n",
    "    topocollections,\n",
    ")\n",
    "from .intersection import Intersection, calculate_needed_intersections\n",
    "from .xtables import ExtensionTable\n",
    "from .databases import label_activity_geocollections\n",
    "from .density import divide_by_area\n",
    "from .lca import (\n",
    "    ExtensionTablesLCA,\n",
    "    GlobalLocLCA, # NR : I added this line to compute the impacts of elementary flows of activities in the [GLO] location.\n",
    "    OneSpatialScaleLCA,\n",
    "    TwoSpatialScalesLCA,\n",
    "    TwoSpatialScalesWithGenericLoadingLCA,\n",
    ")\n",
    "\n",
    "from .base_data import (\n",
    "    create_ecoinvent_collections,\n",
    "    create_restofworlds_collections,\n",
    "    create_world_collections,\n",
    ")\n",
    "from .gis_tasks import calculate_intersection, raster_as_extension_table\n",
    "from .hashing import sha256\n",
    "from .pandarus import import_from_pandarus\n",
    "from .pandarus_remote import PandarusRemote, remote\n",
    "from .utils import (\n",
    "    create_empty_intersection,\n",
    "    get_spatial_dataset_kind,\n",
    "    hash_collection,\n",
    "    import_regionalized_cfs,\n",
    "    reset_all_geo,\n",
    "    reset_geo_meta,\n",
    ")\n",
    "\n",
    "config.metadata.extend(\n",
    "    [extension_tables, geocollections, topocollections, intersections, loadings]\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "855f97c1-a771-4362-be49-0f5588a60021",
   "metadata": {},
   "source": [
    "## base_data.py\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c407cab1-065d-4102-96ac-9459a0c91898",
   "metadata": {},
   "source": [
    "The major changes I made were:\n",
    "1. To remove the links for downloading the \"world\" and \"ecoinvent\" geocollections from an online repository and instead to write the adress of a local file\n",
    "2. To add a geocollection related to the RoW because it was empty.\n",
    "3. To add a tupple (\"world\", k) in the ``Topography`` of the \"world\" geocollection. The explanations are in the section \"3.3 databases.py\"\n",
    "4. I changed the field names of the geocollections to avoid issues when calculating intersections between spatial vectors using geopandas.\n",
    "\n",
    "Add the adress of the different files of the LCI maps for each comments \"NR:\" of the script "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "98e82bd0-1654-46dc-84e2-098840f6bf3b",
   "metadata": {},
   "outputs": [],
   "source": [
    "import bz2\n",
    "import json\n",
    "import os\n",
    "import warnings\n",
    "\n",
    "import rower\n",
    "from bw2data.utils import download_file\n",
    "\n",
    "from . import Topography, cg, geocollections, topocollections\n",
    "from .hashing import sha256\n",
    "\n",
    "COUNTRIES = {\n",
    "    \"AD\",\n",
    "    \"AE\",\n",
    "    \"AF\",\n",
    "    \"AG\",\n",
    "    \"AI\",\n",
    "    \"AL\",\n",
    "    \"AM\",\n",
    "    \"AO\",\n",
    "    \"AQ\",\n",
    "    \"AR\",\n",
    "    \"AS\",\n",
    "    \"AT\",\n",
    "    \"AU\",\n",
    "    \"AW\",\n",
    "    \"AX\",\n",
    "    \"AZ\",\n",
    "    \"BA\",\n",
    "    \"BB\",\n",
    "    \"BD\",\n",
    "    \"BE\",\n",
    "    \"BF\",\n",
    "    \"BG\",\n",
    "    \"BH\",\n",
    "    \"BI\",\n",
    "    \"BJ\",\n",
    "    \"BL\",\n",
    "    \"BM\",\n",
    "    \"BN\",\n",
    "    \"BO\",\n",
    "    \"BR\",\n",
    "    \"BS\",\n",
    "    \"BT\",\n",
    "    \"BW\",\n",
    "    \"BY\",\n",
    "    \"BZ\",\n",
    "    \"CA\",\n",
    "    \"CD\",\n",
    "    \"CF\",\n",
    "    \"CG\",\n",
    "    \"CH\",\n",
    "    \"CI\",\n",
    "    \"CK\",\n",
    "    \"CL\",\n",
    "    \"CM\",\n",
    "    \"CN\",\n",
    "    \"CO\",\n",
    "    \"CR\",\n",
    "    \"CU\",\n",
    "    \"CV\",\n",
    "    \"CW\",\n",
    "    \"CY\",\n",
    "    \"CZ\",\n",
    "    \"DE\",\n",
    "    \"DJ\",\n",
    "    \"DK\",\n",
    "    \"DM\",\n",
    "    \"DO\",\n",
    "    \"DZ\",\n",
    "    \"EC\",\n",
    "    \"EE\",\n",
    "    \"EG\",\n",
    "    \"EH\",\n",
    "    \"ER\",\n",
    "    \"ES\",\n",
    "    \"ET\",\n",
    "    \"FI\",\n",
    "    \"FJ\",\n",
    "    \"FK\",\n",
    "    \"FM\",\n",
    "    \"FO\",\n",
    "    \"FR\",\n",
    "    \"GA\",\n",
    "    \"GB\",\n",
    "    \"GD\",\n",
    "    \"GE\",\n",
    "    \"GG\",\n",
    "    \"GH\",\n",
    "    \"GI\",\n",
    "    \"GL\",\n",
    "    \"GM\",\n",
    "    \"GN\",\n",
    "    \"GQ\",\n",
    "    \"GR\",\n",
    "    \"GS\",\n",
    "    \"GT\",\n",
    "    \"GU\",\n",
    "    \"GW\",\n",
    "    \"GY\",\n",
    "    \"HK\",\n",
    "    \"HM\",\n",
    "    \"HN\",\n",
    "    \"HR\",\n",
    "    \"HT\",\n",
    "    \"HU\",\n",
    "    \"ID\",\n",
    "    \"IE\",\n",
    "    \"IL\",\n",
    "    \"IM\",\n",
    "    \"IN\",\n",
    "    \"IO\",\n",
    "    \"IQ\",\n",
    "    \"IR\",\n",
    "    \"IS\",\n",
    "    \"IT\",\n",
    "    \"JE\",\n",
    "    \"JM\",\n",
    "    \"JO\",\n",
    "    \"JP\",\n",
    "    \"KE\",\n",
    "    \"KG\",\n",
    "    \"KH\",\n",
    "    \"KI\",\n",
    "    \"KM\",\n",
    "    \"KN\",\n",
    "    \"KP\",\n",
    "    \"KR\",\n",
    "    \"KW\",\n",
    "    \"KY\",\n",
    "    \"KZ\",\n",
    "    \"LA\",\n",
    "    \"LB\",\n",
    "    \"LC\",\n",
    "    \"LI\",\n",
    "    \"LK\",\n",
    "    \"LR\",\n",
    "    \"LS\",\n",
    "    \"LT\",\n",
    "    \"LU\",\n",
    "    \"LV\",\n",
    "    \"LY\",\n",
    "    \"MA\",\n",
    "    \"MC\",\n",
    "    \"MD\",\n",
    "    \"ME\",\n",
    "    \"MF\",\n",
    "    \"MG\",\n",
    "    \"MH\",\n",
    "    \"MK\",\n",
    "    \"ML\",\n",
    "    \"MM\",\n",
    "    \"MN\",\n",
    "    \"MO\",\n",
    "    \"MP\",\n",
    "    \"MR\",\n",
    "    \"MS\",\n",
    "    \"MT\",\n",
    "    \"MU\",\n",
    "    \"MV\",\n",
    "    \"MW\",\n",
    "    \"MX\",\n",
    "    \"MY\",\n",
    "    \"MZ\",\n",
    "    \"NA\",\n",
    "    \"NC\",\n",
    "    \"NE\",\n",
    "    \"NF\",\n",
    "    \"NG\",\n",
    "    \"NI\",\n",
    "    \"NL\",\n",
    "    \"NO\",\n",
    "    \"NP\",\n",
    "    \"NR\",\n",
    "    \"NU\",\n",
    "    \"NZ\",\n",
    "    \"OM\",\n",
    "    \"PA\",\n",
    "    \"PE\",\n",
    "    \"PF\",\n",
    "    \"PG\",\n",
    "    \"PH\",\n",
    "    \"PK\",\n",
    "    \"PL\",\n",
    "    \"PM\",\n",
    "    \"PN\",\n",
    "    \"PR\",\n",
    "    \"PS\",\n",
    "    \"PT\",\n",
    "    \"PW\",\n",
    "    \"PY\",\n",
    "    \"QA\",\n",
    "    \"RO\",\n",
    "    \"RS\",\n",
    "    \"RU\",\n",
    "    \"RW\",\n",
    "    \"SA\",\n",
    "    \"SB\",\n",
    "    \"SC\",\n",
    "    \"SD\",\n",
    "    \"SE\",\n",
    "    \"SG\",\n",
    "    \"SH\",\n",
    "    \"SI\",\n",
    "    \"SK\",\n",
    "    \"SL\",\n",
    "    \"SM\",\n",
    "    \"SN\",\n",
    "    \"SO\",\n",
    "    \"SR\",\n",
    "    \"SS\",\n",
    "    \"ST\",\n",
    "    \"SV\",\n",
    "    \"SX\",\n",
    "    \"SY\",\n",
    "    \"SZ\",\n",
    "    \"TC\",\n",
    "    \"TD\",\n",
    "    \"TF\",\n",
    "    \"TG\",\n",
    "    \"TH\",\n",
    "    \"TJ\",\n",
    "    \"TL\",\n",
    "    \"TM\",\n",
    "    \"TN\",\n",
    "    \"TO\",\n",
    "    \"TR\",\n",
    "    \"TT\",\n",
    "    \"TV\",\n",
    "    \"TW\",\n",
    "    \"TZ\",\n",
    "    \"UA\",\n",
    "    \"UG\",\n",
    "    \"UM\",\n",
    "    \"US\",\n",
    "    \"UY\",\n",
    "    \"UZ\",\n",
    "    \"VA\",\n",
    "    \"VC\",\n",
    "    \"VE\",\n",
    "    \"VG\",\n",
    "    \"VI\",\n",
    "    \"VN\",\n",
    "    \"VU\",\n",
    "    \"WF\",\n",
    "    \"WS\",\n",
    "    \"XK\",\n",
    "    \"YE\",\n",
    "    \"ZA\",\n",
    "    \"ZM\",\n",
    "    \"ZW\",\n",
    "}\n",
    "\n",
    "\n",
    "def create_world_collections():\n",
    "    geocollections[\"world\"] = {\n",
    "        \"filepath\": os.path.join(\"\"), #NR: you have to enter the adress of the world map (countries.gpkg)\n",
    "        \"field\": \"isotwolettercode_world\",\n",
    "    }\n",
    "    topocollections[\"world\"] = {\n",
    "        \"geocollection\": \"world\",\n",
    "        \"filepath\": str(cg.faces_fp),\n",
    "        \"field\": \"id\",\n",
    "    }\n",
    "    topo_data = {(\"world\",k): v for k, v in cg.data.items() if k in COUNTRIES} #NR: I added the tupple (\"world\", k) instead of only k\n",
    "    Topography(\"world\").write(topo_data)\n",
    "\n",
    "\n",
    "def create_ecoinvent_collections():\n",
    "    geocollections[\"ecoinvent\"] = {\n",
    "        \"filepath\": os.path.join(\"\"),#NR: you have to enter the adress of the ecoinvent map (all-ecoinvent.gpkg)\n",
    "        \"field\": \"shortname_ecoinvent\",\n",
    "    }\n",
    "    topocollections[\"ecoinvent\"] = {\n",
    "        \"geocollection\": \"ecoinvent\",\n",
    "        \"filepath\": str(cg.faces_fp),\n",
    "        \"field\": \"id\",\n",
    "    }\n",
    "    topo_data = {\n",
    "        (\"ecoinvent\", k): v\n",
    "        for k, v in cg.data.items()\n",
    "        if k != \"__all__\" and \"RoW\" not in k and k not in COUNTRIES\n",
    "    }\n",
    "    Topography(\"ecoinvent\").write(topo_data)\n",
    "\n",
    "\n",
    "def create_restofworlds_collections():\n",
    "    filepath = os.path.join(\n",
    "        rower.DATAPATH, \"ecoinvent generic\", \"rows-topomapping.json.bz2\"\n",
    "    )\n",
    "    with bz2.BZ2File(filepath) as f:\n",
    "        rower_data = json.load(f)\n",
    "\n",
    "    if sha256(cg.faces_fp) != rower_data[\"metadata\"][\"sha256\"]:\n",
    "        warnings.warn(\n",
    "            \"Inconsistent `rower` and `constructive_geometries` packages. Skipping 'RoW' creation\"\n",
    "        )\n",
    "        return\n",
    "\n",
    "    print(\"Creating `rower` 'RoW' geo/topocollections\")\n",
    "    geocollections[\"RoW\"] = {\n",
    "        \"filepath\" : os.path.join(\"\"), #NR: you have to enter the adress of the RoW map (row.gpkg)\n",
    "        \"field\" : \"name_RoW\",\n",
    "        \"kind\" : \"vector\"\n",
    "        }\n",
    "    topocollections[\"RoW\"] = {\n",
    "        \"geocollection\": \"RoW\",\n",
    "        \"filepath\": cg.faces_fp,\n",
    "        \"field\": \"id\",\n",
    "    }\n",
    "    topo_data = {(\"RoW\", k): v for k, v in rower_data[\"data\"]}\n",
    "    Topography(\"RoW\").write(topo_data)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5227c263-3961-48e8-b314-c336a165c1db",
   "metadata": {},
   "source": [
    "## databases.py"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bc7d118c-3e31-4eb2-911e-4a42a692e55c",
   "metadata": {},
   "source": [
    "The major change I did was labeling the location of the activities happening in a country with the tupple (\"world\",\"iso two letters country code\") instead of the string \"iso two letters country code\". \n",
    "``calculate_intersections(engine=geopanda)`` calculates the intersections between two polygons. The results are written as \n",
    "``(\"geocollection 1\", \"id2\") (\"geocollection2\",\"id2\") \"area of the intersection (m2)\"``.\n",
    "As such, if we don't use tupples (\"geocollection 1\", \"id2\") for renaming the locations of the activities of the databases, it leads to trouble populating the different matrices of regionalized LCA.."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1051752d-a420-4c9b-8bd4-ed40ff25c028",
   "metadata": {},
   "outputs": [],
   "source": [
    "import warnings\n",
    "\n",
    "import pyprind\n",
    "from bw2data import Database, databases\n",
    "\n",
    "from . import Topography, geocollections, topocollections\n",
    "from .base_data import COUNTRIES\n",
    "\n",
    "\n",
    "def label_activity_geocollections(name):\n",
    "    \"\"\"Add geocollections to activity ``location`` fields.\n",
    "\n",
    "    ``name`` is the name of an existing LCI database.\"\"\"\n",
    "    assert name in databases, \"{} not found\".format(name)\n",
    "    assert \"world\" in geocollections, \"Please run `create_world_collections` first\"\n",
    "\n",
    "    #NR: I added a world set in order to be able after to label databases locations with the tuple (\"world\", \"two letter code of country\")\n",
    "    world = (\n",
    "        {x[1] for x in Topography(\"world\").load()}\n",
    "        if \"world\" in topocollections\n",
    "        else set()\n",
    "    )\n",
    "    \n",
    "    ecoinvent = (\n",
    "        {x[1] for x in Topography(\"ecoinvent\").load()}\n",
    "        if \"ecoinvent\" in topocollections\n",
    "        else set()\n",
    "    )\n",
    "    RoWs = (\n",
    "        {x[1] for x in Topography(\"RoW\").load()} if \"RoW\" in topocollections else set()\n",
    "    )\n",
    "\n",
    "    db = Database(name)\n",
    "    searchable = db.metadata.get(\"searchable\")\n",
    "    if searchable:\n",
    "        db.make_unsearchable()\n",
    "\n",
    "    found_geocollections = set()\n",
    "\n",
    "    locations = {x[\"location\"] for x in db}\n",
    "    assert \"RoW\" not in locations, \"`RoW` found; use `rower` to label Rest-of-Worlds\"\n",
    "\n",
    "    for act in pyprind.prog_bar(db):\n",
    "        if isinstance(act[\"location\"], tuple):\n",
    "            found_geocollections.add(act[\"location\"][0])\n",
    "        elif act[\"location\"] in COUNTRIES: \n",
    "            act[\"location\"] = (\"world\", act[\"location\"]) #NR: labeling databases locations with the tuple (\"world\", \"iso two letter code of country\")\n",
    "            act.save()\n",
    "            found_geocollections.add(\"world\")\n",
    "        elif act[\"location\"] == \"GLO\":\n",
    "            found_geocollections.add(\"world\") #NR: I did not label it with a tupple (\"world\",\"GLO\") as the other world location because the GLO location is special (maybe we could modify it)\n",
    "        elif act[\"location\"] in RoWs:\n",
    "            act[\"location\"] = (\"RoW\", act[\"location\"])\n",
    "            act.save()\n",
    "            found_geocollections.add(\"RoW\")\n",
    "        elif act[\"location\"] in ecoinvent:\n",
    "            act[\"location\"] = (\"ecoinvent\", act[\"location\"])\n",
    "            act.save()\n",
    "            found_geocollections.add(\"ecoinvent\")\n",
    "        else:\n",
    "            warnings.warn(\n",
    "                (\n",
    "                    \"Location {} in {} not understood; please add geocollection\"\n",
    "                    \" manually, and add to databases[name]['geocollections']\"\n",
    "                ).format(act[\"location\"], act.key)\n",
    "            )\n",
    "\n",
    "    if searchable:\n",
    "        db.make_searchable()\n",
    "    db.process()\n",
    "\n",
    "    db.metadata[\"regionalized\"] = True\n",
    "    db.metadata[\"geocollections\"] = sorted(found_geocollections)\n",
    "    databases.flush()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d832c61e-2ede-407d-93c3-5697f514ffa3",
   "metadata": {},
   "source": [
    "## lca\\\\__init__.py"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "deabca0f-c698-4477-828d-a804cd6b2740",
   "metadata": {},
   "source": [
    "I added a new class ``GlobalLocLCA`` that allow the evaluation of the impacts of elementary flows of activities happening in the [GLO] location"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bfb917f2-7ad1-4593-8b6b-798b83434c3d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# -*- coding: utf-8 -*-\n",
    "from .extension_tables import ExtensionTablesLCA\n",
    "from .global_loc import GlobalLocLCA\n",
    "from .one_spatial_scale import OneSpatialScaleLCA\n",
    "from .two_spatial_scales import TwoSpatialScalesLCA\n",
    "from .two_spatial_scales_weighting import TwoSpatialScalesWithGenericLoadingLCA"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1db2c72f-2622-4f00-a526-952400e61e16",
   "metadata": {},
   "source": [
    "## lca\\\\base_class.py"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f8afcee1-a616-4c67-b36d-559a9032911a",
   "metadata": {},
   "source": [
    "I added a new module in the ``RegionalizationBase`` class : ``create_glob_characterization_matrix``, which will allow the computation of elementary flows of activities in [GLO] location. This new module is the same as ``create_regionalized_characterization_matrix``."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2313edc9-e046-40f2-ad4a-f1f1f48990e8",
   "metadata": {},
   "outputs": [],
   "source": [
    "import itertools\n",
    "from functools import partial\n",
    "\n",
    "import matrix_utils as mu\n",
    "import numpy as np\n",
    "from bw2calc.lca import LCA\n",
    "from bw2data import Database, Method, databases, get_activity, methods\n",
    "from scipy.sparse import coo_matrix, csr_matrix\n",
    "\n",
    "from ..errors import MissingIntersection, SiteGenericMethod, UnprocessedDatabase\n",
    "from ..export import create_geodataframe\n",
    "from ..intersection import Intersection\n",
    "from ..meta import intersections\n",
    "from ..utils import dp\n",
    "\n",
    "\n",
    "def get_dependent_databases(demand_dict):\n",
    "    \"\"\"Demand can be activitiy ids or tuple keys.\"\"\"\n",
    "    db_labels = [\n",
    "        x[0] if isinstance(x, tuple) else get_activity(x)[\"database\"]\n",
    "        for x in demand_dict\n",
    "    ]\n",
    "    return set.union(*[Database(label).find_graph_dependents() for label in db_labels])\n",
    "\n",
    "\n",
    "def annotate_flow(flow_id, _):\n",
    "    flow = get_activity(id=flow_id)\n",
    "    return {\n",
    "        \"flow_name\": flow.get(\"name\", \"\"),\n",
    "        \"flow_unit\": flow.get(\"unit\", \"\"),\n",
    "        \"flow_categories\": str(flow.get(\"categories\", \"\")),\n",
    "    }\n",
    "\n",
    "\n",
    "class RegionalizationBase(LCA):\n",
    "    def __init__(self, demand, *args, **kwargs):\n",
    "        self.databases = get_dependent_databases(demand)\n",
    "        self.extra_data_objs = kwargs.pop(\"extra_data_objs\", [])\n",
    "        super(RegionalizationBase, self).__init__(demand, *args, **kwargs)\n",
    "\n",
    "    def get_inventory_geocollections(self):\n",
    "        \"\"\"Get the set of all needed inventory geocollections.\n",
    "\n",
    "        Raise UnprocessedDatabase if any database is missing the required metadata.\"\"\"\n",
    "        missing, present = [], set()\n",
    "        for database in self.databases:\n",
    "            if \"geocollections\" not in databases[database]:\n",
    "                missing.append(database)\n",
    "            else:\n",
    "                present.update(set(databases[database][\"geocollections\"]))\n",
    "        if missing:\n",
    "            raise UnprocessedDatabase(\n",
    "                \"Database(s) {} don't specify their geocollections.\".format(missing)\n",
    "            )\n",
    "        return present\n",
    "\n",
    "    def get_ia_geocollections(self):\n",
    "        \"\"\"Retrieve the geocollections linked to the impact assessment method\"\"\"\n",
    "        ia_gc = set(methods[self.method][\"geocollections\"])\n",
    "        if not ia_gc:\n",
    "            raise SiteGenericMethod\n",
    "        return ia_gc\n",
    "\n",
    "    def create_inventory_mapping_matrix(self):\n",
    "        \"\"\"Get inventory mapping matrix, **M**, which maps inventory activities to inventory locations. Rows are inventory activities and columns are inventory spatial units.\n",
    "\n",
    "        Uses ``self.technosphere_mm.row_mapper`` and ``self.databases``.\n",
    "\n",
    "        Creates ``self.inv_mapping_mm``, ``self.inv_mapping_matrix``, and ``self.dicts.inv_spatial``/\n",
    "\n",
    "        \"\"\"\n",
    "        self.inv_mapping_mm = mu.MappedMatrix(\n",
    "            packages=[\n",
    "                dp(Database(x).filepath_processed()) for x in self.databases\n",
    "            ] + self.extra_data_objs,\n",
    "            matrix=\"inv_geomapping_matrix\",\n",
    "            use_arrays=self.use_arrays,\n",
    "            use_distributions=self.use_distributions,\n",
    "            seed_override=self.seed_override,\n",
    "            row_mapper=self.technosphere_mm.col_mapper,\n",
    "        )\n",
    "        self.inv_mapping_matrix = self.inv_mapping_mm.matrix\n",
    "        self.dicts.inv_spatial = partial(self.inv_mapping_mm.col_mapper.to_dict)\n",
    "\n",
    "    def needed_intersections(self):\n",
    "        \"\"\"Figure out which ``Intersection`` objects are needed bsed on ``self.inventory_geocollections`` and ``self.ia_geocollections``.\n",
    "\n",
    "        Raise ``MissingIntersection`` if an intersection is required, but not available.\"\"\"\n",
    "        required = list(\n",
    "            itertools.product(self.inventory_geocollections, self.ia_geocollections)\n",
    "            )\n",
    "        for obj in required:\n",
    "            if obj not in intersections:\n",
    "                raise MissingIntersection(\n",
    "                    \"Intersection {} needed but not found\".format(obj)\n",
    "                )\n",
    "        return required\n",
    "\n",
    "    def create_geo_transform_matrix(self):\n",
    "        \"\"\"Get geographic transform matrix **G**, which gives the intersecting areas of inventory and impact assessment spatial units. Rows are inventory spatial units, and columns are impact assessment spatial units.\n",
    "\n",
    "        Uses ``self.inv_spatial_dict`` and ``self.ia_spatial_dict``.\n",
    "\n",
    "        Returns:\n",
    "            * ``geo_transform_params``: Parameter array with row/col of inventory and IA locations\n",
    "            * ``geo_transform_matrix``: The matrix **G**\n",
    "\n",
    "        \"\"\"\n",
    "        self.geo_transform_mm = mu.MappedMatrix(\n",
    "            packages=[\n",
    "                dp(Intersection(name).filepath_processed())\n",
    "                for name in self.needed_intersections()\n",
    "            ] + self.extra_data_objs,\n",
    "            matrix=\"intersection_matrix\",\n",
    "            use_arrays=self.use_arrays,\n",
    "            use_distributions=self.use_distributions,\n",
    "            seed_override=self.seed_override,\n",
    "            col_mapper=self.reg_cf_mm.row_mapper,\n",
    "            row_mapper=self.inv_mapping_mm.col_mapper,\n",
    "        )\n",
    "        self.geo_transform_matrix = self.geo_transform_mm.matrix\n",
    "\n",
    "    def create_regionalized_characterization_matrix(self, row_mapper=None):\n",
    "        \"\"\"Get regionalized characterization matrix, **R**, which gives location- and biosphere flow-specific characterization factors.\n",
    "\n",
    "        Rows are impact assessment spatial units, and columns are biosphere flows. However, we build it transverse and transpose it, as the characterization matrix indices are provided that way.\n",
    "\n",
    "        Uses ``self._biosphere_dict`` and ``self.method``.\n",
    "\n",
    "        Returns:\n",
    "            * ``reg_cf_params``: Parameter array with row/col of IA locations/biosphere flows\n",
    "            * ``ia_spatial_dict``: Dictionary linking impact assessment locations to matrix rows\n",
    "            * ``reg_cf_matrix``: The matrix **R**\n",
    "\n",
    "        \"\"\"\n",
    "        self.reg_cf_mm = mu.MappedMatrix(\n",
    "            packages=[\n",
    "                dp(Method(self.method).filepath_processed())\n",
    "            ] + self.extra_data_objs,\n",
    "            matrix=\"characterization_matrix\",\n",
    "            use_arrays=self.use_arrays,\n",
    "            use_distributions=self.use_distributions,\n",
    "            seed_override=self.seed_override,\n",
    "            col_mapper=self.biosphere_mm.row_mapper,\n",
    "            row_mapper=row_mapper,\n",
    "            transpose=True,\n",
    "        )\n",
    "        self.reg_cf_matrix = self.reg_cf_mm.matrix\n",
    "        if row_mapper is None:\n",
    "            self.dicts.ia_spatial = partial(self.reg_cf_mm.row_mapper.to_dict)\n",
    "\n",
    "    def create_glob_characterization_matrix(self, row_mapper=None): # NR : I added this module in order to characterize the elementary flows happening at the [GLO] location\n",
    "            \"\"\" Creating a characterization matrix for elementary flows of the [GLO] location\"\"\"\n",
    "            self.glob_cf_mm = mu.MappedMatrix(\n",
    "                packages=[\n",
    "                    dp(Method(self.method).filepath_processed())\n",
    "                ] + self.extra_data_objs,\n",
    "                matrix=\"characterization_matrix\",\n",
    "                use_arrays=self.use_arrays,\n",
    "                use_distributions=self.use_distributions,\n",
    "                seed_override=self.seed_override,\n",
    "                col_mapper=self.biosphere_mm.row_mapper,\n",
    "                row_mapper=row_mapper,\n",
    "                transpose=True,\n",
    "            )\n",
    "            self.glob_cf_matrix = self.glob_cf_mm.matrix\n",
    "            if row_mapper is None:\n",
    "                self.dicts.ia_spatial = partial(self.glob_cf_mm.row_mapper.to_dict)\n",
    "\n",
    "    def create_loading_matrix(self):\n",
    "        \"\"\"Get diagonal regionalized loading matrix, **L**, which gives location-specific background loading factors. Dimensions are impact assessment spatial units.\n",
    "\n",
    "        Uses ``self.dicts.ia_spatial``.\n",
    "\n",
    "        \"\"\"\n",
    "        self.loading_mm = mu.MappedMatrix(\n",
    "            packages=[\n",
    "                dp(self.loading.filepath_processed())\n",
    "            ] + self.extra_data_objs,\n",
    "            matrix=\"loading_matrix\",\n",
    "            use_arrays=self.use_arrays,\n",
    "            use_distributions=self.use_distributions,\n",
    "            seed_override=self.seed_override,\n",
    "            diagonal=True,\n",
    "            row_mapper=self.reg_cf_mm.row_mapper,\n",
    "        )\n",
    "        self.loading_matrix = self.loading_mm.matrix\n",
    "\n",
    "    def _results_new_scale(self, matrix, flow):\n",
    "        # self.fix_spatial_dictionaries()\n",
    "        if flow is not None:\n",
    "            try:\n",
    "                row_index = self.biosphere_dict[flow]\n",
    "                matrix = matrix[row_index, :]\n",
    "            except KeyError:\n",
    "                raise ValueError(\"Flow {} not in biosphere dict\".format(flow))\n",
    "        else:\n",
    "            # using matrix.sum() converts to dense numpy matrix\n",
    "            nc = matrix.shape[0]\n",
    "            summer = csr_matrix(\n",
    "                (np.ones(nc), np.arange(nc), np.array((0, nc), dtype=int))\n",
    "            )\n",
    "            matrix = summer * matrix\n",
    "        return matrix\n",
    "\n",
    "    def results_ia_spatial_scale(self):\n",
    "        raise NotImplementedError(\"Must be defined in subclasses\")\n",
    "\n",
    "    def results_inv_spatial_scale(self):\n",
    "        raise NotImplementedError(\"Must be defined in subclasses\")\n",
    "\n",
    "    def __geodataframe(\n",
    "        self, matrix, sum_flows, annotate_flows, col_dict, used_geocollections, cutoff\n",
    "    ):\n",
    "        if sum_flows:\n",
    "            matrix = coo_matrix(matrix.sum(axis=0))\n",
    "            annotate_flows = None\n",
    "        elif annotate_flows:\n",
    "            annotate_flows = annotate_flow\n",
    "\n",
    "        return create_geodataframe(\n",
    "            matrix=matrix,\n",
    "            used_geocollections=used_geocollections,\n",
    "            row_dict=self.dicts.biosphere,\n",
    "            col_dict=col_dict,\n",
    "            attribute_adder=annotate_flows,\n",
    "            cutoff=cutoff,\n",
    "        )\n",
    "\n",
    "    def geodataframe_xtable_spatial_scale(\n",
    "        self, sum_flows=True, annotate_flows=None, cutoff=None\n",
    "    ):\n",
    "        if not hasattr(self, \"results_xtable_spatial_scale\"):\n",
    "            raise NotImplementedError\n",
    "\n",
    "        matrix = self.results_xtable_spatial_scale()\n",
    "        return self.__geodataframe(\n",
    "            matrix=matrix,\n",
    "            sum_flows=sum_flows,\n",
    "            annotate_flows=annotate_flows,\n",
    "            col_dict=self.dicts.xtable_spatial,\n",
    "            used_geocollections=self.xtable_geocollections,\n",
    "            cutoff=cutoff,\n",
    "        )\n",
    "\n",
    "    def geodataframe_ia_spatial_scale(\n",
    "        self, sum_flows=True, annotate_flows=None, cutoff=None\n",
    "    ):\n",
    "        matrix = self.results_ia_spatial_scale()\n",
    "        return self.__geodataframe(\n",
    "            matrix=matrix,\n",
    "            sum_flows=sum_flows,\n",
    "            annotate_flows=annotate_flows,\n",
    "            col_dict=self.dicts.ia_spatial,\n",
    "            used_geocollections=self.ia_geocollections,\n",
    "            cutoff=cutoff,\n",
    "        )\n",
    "\n",
    "    def geodataframe_inv_spatial_scale(\n",
    "        self, sum_flows=True, annotate_flows=None, cutoff=None\n",
    "    ):\n",
    "        matrix = self.results_inv_spatial_scale()\n",
    "        return self.__geodataframe(\n",
    "            matrix=matrix,\n",
    "            sum_flows=sum_flows,\n",
    "            annotate_flows=annotate_flows,\n",
    "            col_dict=self.dicts.inv_spatial,\n",
    "            used_geocollections=self.inventory_geocollections,\n",
    "            cutoff=cutoff,\n",
    "        )"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "95647809-9357-402f-bd76-bd52396595ab",
   "metadata": {},
   "source": [
    "## lca\\\\global_loc.py"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "75eb1cf7-d1a2-4b29-9e2d-26561e826cb2",
   "metadata": {},
   "source": [
    "The ``GlobalLocLCA`` class will account for elementary flows of activities in the [GLO] location. It uses the RegionalizationBase class. It is a copy of the OneSpatialScaleLCA class with some modifications, e.g. removal of the error impeding the computation of the LCA with shared spatial scale when there are two spatial scales and modification of LCIA_calculation with the use of ``self.glob_cf_matrix``. The ``glob_cf_matrix`` is only filled with CFs related to the [GLO] location."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7f9d352c-1687-4136-b027-e017c44037bc",
   "metadata": {},
   "outputs": [],
   "source": [
    "from bw2data import methods\n",
    "\n",
    "from .base_class import RegionalizationBase\n",
    "\n",
    "class GlobalLocLCA(RegionalizationBase):\n",
    "    matrix_labels = [\n",
    "        \"biosphere_mm\",\n",
    "        \"inv_mapping_mm\",\n",
    "        \"glob_cf_mm\",\n",
    "        \"technosphere_mm\",\n",
    "    ]\n",
    "    \n",
    "    def __init__(self, *args, **kwargs):\n",
    "        \"\"\"Perform LCA calculation, for the elementary flows at the global scale, which are not computed in the TwoSpatialScalesLCA\n",
    "        \"\"\"\n",
    "        super(GlobalLocLCA, self).__init__(*args, **kwargs)\n",
    "        if self.method not in methods:\n",
    "            raise ValueError(\"Must pass valid `method` name\")\n",
    "        self.inventory_geocollections = self.get_inventory_geocollections()\n",
    "        self.ia_geocollections = self.get_ia_geocollections()\n",
    "\n",
    "    def load_lcia_data(self):\n",
    "        self.create_inventory_mapping_matrix()\n",
    "        self.create_glob_characterization_matrix(self.inv_mapping_mm.col_mapper)\n",
    "  \n",
    "    def lcia_calculation(self):\n",
    "        \"\"\"Do LCA calculation for elementary flows at the global scale.\n",
    "\n",
    "        Creates ``self.characterized_inventory``.\n",
    "\n",
    "        \"\"\"\n",
    "        self.characterized_inventory = (\n",
    "            self.inv_mapping_matrix * self.glob_cf_matrix\n",
    "        ).T.multiply(self.inventory)\n",
    "\n",
    "    def results_ia_spatial_scale(self):\n",
    "        raise NotImplementedError(\"No separate IA spatial scale\")\n",
    "\n",
    "    def results_inv_spatial_scale(self):\n",
    "        if not hasattr(self, \"characterized_inventory\"):\n",
    "            raise ValueError(\"Must do lcia calculation first\")\n",
    "        return self.reg_cf_matrix.T.multiply(self.inventory * self.inv_mapping_matrix)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e05c6183-f927-484d-95fe-2322b3316040",
   "metadata": {},
   "source": [
    "# Modification of the ``bw2_lcimpact``"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3987c9c7-7e4f-4d76-9419-03a182bc2ded",
   "metadata": {},
   "source": [
    "## water.py"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6cb03590-c240-4d64-bcc8-d0931d640fe1",
   "metadata": {},
   "source": [
    "I renamed the field name \"id\" to \"id_water_eq_sw\" for the geocollections related to the impacts of water consumption on ecosystem quality to avoid conflicts of names."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "778e9d07-4be5-43ec-95c8-76662af9b270",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "from .base import LCIA, data_dir, fiona, geocollections, regionalized\n",
    "\n",
    "\"\"\"When is water not water? When it is in water!\n",
    "\n",
    "We are assessing freshwater consumption, and so some water flows are excluded.\"\"\"\n",
    "\n",
    "SURFACE_WATER = [\n",
    "    (\"Fresh water (obsolete)\", (\"water\", \"surface water\"), -1),\n",
    "    (\"Water\", (\"water\",), -1),\n",
    "    (\"Water\", (\"water\", \"surface water\"), -1),\n",
    "    (\"Water, cooling, unspecified natural origin\", (\"natural resource\", \"in water\"), 1),\n",
    "    (\"Water, lake\", (\"natural resource\", \"in water\"), 1),\n",
    "    (\"Water, river\", (\"natural resource\", \"in water\"), 1),\n",
    "    (\n",
    "        \"Water, turbine use, unspecified natural origin\",\n",
    "        (\"natural resource\", \"in water\"),\n",
    "        1,\n",
    "    ),\n",
    "    (\"Water, unspecified natural origin\", (\"natural resource\", \"in water\"), 1),\n",
    "]\n",
    "\n",
    "GROUND_WATER = [\n",
    "    (\"Water\", (\"water\", \"ground-\"), -1),\n",
    "    # ('Water', ('water', 'ground-, long-term')),\n",
    "    (\"Water, unspecified natural origin\", (\"natural resource\", \"in ground\"), 1),\n",
    "    (\"Water, well, in ground\", (\"natural resource\", \"in water\"), 1),\n",
    "]\n",
    "\n",
    "\n",
    "class Water(LCIA):\n",
    "    def _water_flows(self, kind=\"all\"):\n",
    "        mapping = {\n",
    "            \"all\": SURFACE_WATER + GROUND_WATER,\n",
    "            \"surface\": SURFACE_WATER,\n",
    "            \"ground\": GROUND_WATER,\n",
    "        }\n",
    "        flows = mapping[kind]\n",
    "\n",
    "        for act in self.db:\n",
    "            name, categories = act[\"name\"], tuple(act[\"categories\"])\n",
    "            for x, y, sign in flows:\n",
    "                if name == x and categories == y:\n",
    "                    yield act.key, sign\n",
    "\n",
    "\n",
    "class WaterHumanHealthMarginal(Water):\n",
    "    vector_ds = os.path.join(data_dir, \"water_hh.gpkg\")\n",
    "    geocollection = \"watersheds-hh\"\n",
    "    column = \"HH\"\n",
    "\n",
    "    name = (\"LC-IMPACT\", \"Water Use\", \"Human Health\", \"Marginal\")\n",
    "    global_cf = 1.8e-7\n",
    "    unit = \"DALY/m3\"\n",
    "    description = \"\"\"The impact assessment method for assessing water consumption concerning the area of protection of human health is described based on Pfister et al. (2009) for the impact pathway (marginal CF), Pfister and Hellweg (2011) for uncertainty assessment, and Pfister and Bayer (2013) for average CFs.\n",
    "\n",
    "    Only the 'certain' level of uncertainty is provided.\"\"\"\n",
    "    url = \"http://lc-impact.eu/human-health-water-stress\"\n",
    "\n",
    "    @regionalized\n",
    "    def setup_geocollections(self):\n",
    "        if self.geocollection not in geocollections:\n",
    "            geocollections[self.geocollection] = {\n",
    "                \"filepath\": self.vector_ds,\n",
    "                \"field\": \"BAS34S_ID\",\n",
    "            }\n",
    "\n",
    "    def global_cfs(self):\n",
    "        for key, sign in self._water_flows():\n",
    "            yield ((key, self.global_cf * sign, \"GLO\"))\n",
    "\n",
    "    @regionalized\n",
    "    def regional_cfs(self):\n",
    "        water_flows = list(self._water_flows())\n",
    "\n",
    "        for obj in self.global_cfs():\n",
    "            yield obj\n",
    "\n",
    "        with fiona.Env():\n",
    "            with fiona.open(self.vector_ds) as src:\n",
    "                for feat in src:\n",
    "                    for key, sign in water_flows:\n",
    "                        yield (\n",
    "                            key,\n",
    "                            feat[\"properties\"][self.column]\n",
    "                            * 1e-9\n",
    "                            * sign,  # Convert km3 to m3\n",
    "                            (self.geocollection, feat[\"properties\"][\"BAS34S_ID\"]),\n",
    "                        )\n",
    "\n",
    "\n",
    "class WaterHumanHealthAverage(WaterHumanHealthMarginal):\n",
    "    name = (\"LC-IMPACT\", \"Water Use\", \"Human Health\", \"Average\")\n",
    "    global_cf = 1.3e-7\n",
    "    column = \"HH_AVG\"\n",
    "\n",
    "\n",
    "class WaterEcosystemQualityCertain(Water):\n",
    "    vector_ds = os.path.join(data_dir, \"water_eq_sw_core.gpkg\")\n",
    "    geocollection = \"watersheds-eq-sw-certain\"\n",
    "    column = \"val\"\n",
    "\n",
    "    name = (\n",
    "        \"LC-IMPACT\",\n",
    "        \"Water Use\",\n",
    "        \"Ecosystem Quality\",\n",
    "        \"Surface Water\",\n",
    "        \"Marginal\",\n",
    "        \"Certain\",\n",
    "    )\n",
    "    global_cf = 1.63e-13\n",
    "    unit = \"PDF·yr/m3\"\n",
    "    description = \"\"\"The description of the impact assessment approach for quantifying impacts from water consumption on biodiversity is based on Verones et al. (submitted), which is a continuation from Verones et al. (2013a) and Verones et al. (2013b), as well as Chaudhary et al. (2015).\"\"\"\n",
    "    url = \"http://lc-impact.eu/ecosystem-quality-water-stress\"\n",
    "\n",
    "    _flows_label = \"surface\"\n",
    "\n",
    "    @regionalized\n",
    "    def setup_geocollections(self):\n",
    "        if self.geocollection not in geocollections:\n",
    "            geocollections[self.geocollection] = {\n",
    "                \"filepath\": self.vector_ds,\n",
    "                \"field\": \"id_water_eq_sw\", #NR: I added \"_water_eq_sw\" to the field name\n",
    "            }\n",
    "\n",
    "    def global_cfs(self):\n",
    "        for key, sign in self._water_flows(self._flows_label):\n",
    "            yield ((key, self.global_cf * sign, \"GLO\"))\n",
    "\n",
    "    @regionalized\n",
    "    def regional_cfs(self):\n",
    "        water_flows = list(self._water_flows(self._flows_label))\n",
    "\n",
    "        for obj in self.global_cfs():\n",
    "            yield obj\n",
    "\n",
    "        with fiona.Env():\n",
    "            with fiona.open(self.vector_ds) as src:\n",
    "                for feat in src:\n",
    "                    for key, sign in water_flows:\n",
    "                        yield (\n",
    "                            key,\n",
    "                            feat[\"properties\"][self.column] * sign,\n",
    "                            (self.geocollection, feat[\"properties\"][\"id_water_eq_sw\"]), #NR: I added \"_water_eq_sw\" to the field name\n",
    "                        )\n",
    "\n",
    "\n",
    "class WaterEcosystemQualityAll(WaterEcosystemQualityCertain):\n",
    "    vector_ds = os.path.join(data_dir, \"water_eq_sw_extended.gpkg\")\n",
    "    geocollection = \"watersheds-eq-sw-all\"\n",
    "\n",
    "    name = (\n",
    "        \"LC-IMPACT\",\n",
    "        \"Water Use\",\n",
    "        \"Ecosystem Quality\",\n",
    "        \"Surface Water\",\n",
    "        \"Marginal\",\n",
    "        \"All\",\n",
    "    )\n",
    "    global_cf = 1.65e-13\n",
    "\n",
    "    _flows_label = \"all\"\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
